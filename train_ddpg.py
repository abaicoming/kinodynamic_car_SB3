import os
import time
import shutil
import argparse
import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym
import torch
from matplotlib.patches import Circle

import matplotlib.cm as cm
from matplotlib.colors import to_rgba

# ===== SB3 =====
from stable_baselines3 import DDPG, TD3
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.noise import NormalActionNoise
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecNormalize
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.vec_env import sync_envs_normalization

# è§¦å‘ç¯å¢ƒæ³¨å†Œï¼ˆç¡®ä¿ __init__.py æ³¨å†Œäº† KinematicCar-v0ï¼‰
import kinodynamic_car_SB3.dubins_env  # noqa: F401

# ========= å…¨å±€é…ç½®ï¼ˆé»˜è®¤å€¼ï¼Œå¯è¢«å‘½ä»¤è¡Œè¦†ç›–ï¼‰ =========
ENV_ID = "KinematicCar-v0"
SEED = 42
ALG = "DDPG"                 # or "TD3"
Algo = DDPG if ALG == "DDPG" else TD3
N_ENVS = 24                  # å¹¶è¡Œç¯å¢ƒä¸ªæ•°
EVAL_ENVS = 8                # è¯„ä¼°å¹¶è¡Œç¯å¢ƒä¸ªæ•°
TOTAL_STEPS = 300_000

# ----------------  å¤‡ä»½æºç ï¼ˆç¯å¢ƒåŒ… + æœ¬è„šæœ¬ï¼‰----------------
def backup_sources(log_dir: str):
    src_env_dir = os.path.join(os.path.dirname(__file__), "dubins_env")
    dst_env_dir = os.path.join(log_dir, "dubins_env")
    try:
        if not os.path.exists(dst_env_dir):
            shutil.copytree(src_env_dir, dst_env_dir)
            print(f"[backup] env -> {dst_env_dir}")
        else:
            print(f"[backup] env already exists -> {dst_env_dir}")
    except Exception as e:
        print(f"[WARN] copy env failed: {e}")

    src_script = os.path.abspath(__file__)
    dst_script = os.path.join(log_dir, "train_ddpg_backup.py")
    try:
        shutil.copy2(src_script, dst_script)
        print(f"[backup] script -> {dst_script}")
    except Exception as e:
        print(f"[WARN] copy script failed: {e}")

# ============ VecEnv å·¥å‚å‡½æ•° ============

def make_env_train(rank: int, log_dir: str):
    def _thunk():
        env = gym.make(ENV_ID)
        env = Monitor(env, log_dir)
        env.reset(seed=SEED + rank)
        return env
    return _thunk

def make_env_eval_for_callback(rank: int, log_dir: str):
    def _thunk():
        return Monitor(gym.make(ENV_ID), log_dir)
    return _thunk

def make_env_eval():
    def _thunk():
        return Monitor(gym.make(ENV_ID))
    return _thunk

def make_env_for_traj():
    def _thunk():
        return Monitor(gym.make(ENV_ID))
    return _thunk

def visualize_critic_contour(
    log_dir: str,
    goal_pose: tuple,
    x_range: tuple = (-6.0, 6.0),
    y_range: tuple = (-6.0, 6.0),
    grid_N: int = 81,                  # ç½‘æ ¼å¯†åº¦ï¼ˆå¥‡æ•°å±…ä¸­ï¼Œå»ºè®® 51/81/101ï¼‰
    start_theta: float = 0.0,          # èµ·ç‚¹æœå‘å›ºå®šå€¼
    deterministic: bool = True,
    use_min_q_for_td3: bool = True,    # TD3 ç”¨ min(Q1, Q2)
    cmap: str = "viridis",
    title: str = "Critic Q(s, Ï€(s)) field",
):
    """
    å›ºå®š goalï¼Œåœ¨ (x_range, y_range) ä¸Šå‡åŒ€é‡‡æ ·èµ·ç‚¹ (x, y, start_theta)ï¼Œ
    å¯¹æ¯ä¸ªèµ·ç‚¹ç”¨å½“å‰ç­–ç•¥ Ï€(s) å¾—åˆ°åŠ¨ä½œ aï¼Œå†ç”¨ critic è¯„ä¼° Q(s, a)ï¼Œç»˜åˆ¶ç­‰å€¼é¢ã€‚
    """
    # ---- è·¯å¾„ / è®¾å¤‡ / ç¯å¢ƒ & å½’ä¸€åŒ– ----
    alg_name = ALG
    model_path = os.path.join(log_dir, f"{alg_name}_kinematic_model")
    vecnorm_path = os.path.join(log_dir, f"{alg_name}_vecnorm.pkl")
    plot_dir = os.path.join(log_dir, "plots")
    os.makedirs(plot_dir, exist_ok=True)

    # å•ç¯å¢ƒ + è½½å…¥ vecnorm ç»Ÿè®¡
    traj_venv = DummyVecEnv([make_env_for_traj()])
    traj_venv = VecNormalize.load(vecnorm_path, traj_venv)
    traj_venv.training = False
    traj_venv.norm_reward = False

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = Algo.load(model_path, device=device)
    policy = model.policy  # æ–¹ä¾¿ä¸‹é¢ç›´æ¥ç”¨ actor/critic
    base_env = traj_venv.venv.envs[0].unwrapped

    # ---- ç”Ÿæˆç½‘æ ¼ ----
    xs = np.linspace(x_range[0], x_range[1], grid_N)
    ys = np.linspace(y_range[0], y_range[1], grid_N)
    Qmap = np.full((grid_N, grid_N), np.nan, dtype=np.float32)

    # ---- éå†ç½‘æ ¼å¹¶è¯„ä¼° Q ----
    for ix, x in enumerate(xs):
        for iy, y in enumerate(ys):
            start = (float(x), float(y), float(start_theta))
            goal  = (float(goal_pose[0]), float(goal_pose[1]), float(goal_pose[2]))

            # æŠŠâ€œä¸‹æ¬¡ resetâ€çš„èµ·ç»ˆç‚¹ä¼ å…¥ç¯å¢ƒ
            traj_venv.env_method("set_start_goal_for_next_reset", start=start, goal=goal)
            obs = traj_venv.reset()  # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„ obs å·²ç»è¿‡ VecNormalize å¤„ç†

            # ç”¨ç­–ç•¥å¾—åˆ°åŠ¨ä½œï¼ˆdeterministic=True æ—¶ç”¨å‡å€¼ï¼‰
            with torch.no_grad():
                obs_t = torch.as_tensor(obs, device=device).float().unsqueeze(0)  # [1, obs_dim]
                # ç”¨ actor å…ˆå‡ºä¸€ä¸ªåŠ¨ä½œ
                act_t = model.policy.actor(obs_t)

                critic = getattr(model, "critic", None)
                if critic is None:
                    critic = getattr(model, "critic_target", None)
                assert critic is not None, "No critic found on the model."

                # ç»Ÿä¸€æ‹¿åˆ° q1 / q2ï¼ˆå¦‚æœæœ‰ï¼‰
                q1_t, q2_t = None, None
                if hasattr(critic, "q1_forward"):                 # TD3 / SAC çš„ ContinuousCritic
                    q1_t = critic.q1_forward(obs_t, act_t)        # [1, 1]
                    if hasattr(critic, "q2_forward"):
                        q2_t = critic.q2_forward(obs_t, act_t)    # [1, 1]
                else:
                    q1_t = critic(obs_t, act_t)                   # DDPG çš„ QNetwork: [1, 1] æˆ– [1]
                    # æœ‰äº›ç‰ˆæœ¬å¯èƒ½è¿”å› (tensor,) è¿™ç§å¥‡æ€ªåŒ…è£…ï¼Œç»Ÿä¸€å–ç¬¬ 0 é¡¹
                    if isinstance(q1_t, (tuple, list)):
                        q1_t = q1_t[0]

                # å–æœ€ç»ˆç”¨äºç”»ç­‰é«˜çº¿çš„ Q å€¼ï¼ˆTD3 å– min(Q1, Q2)ï¼ŒDDPG å°±æ˜¯ Q1ï¼‰
                if q2_t is not None:
                    q_val = torch.min(q1_t, q2_t)
                else:
                    q_val = q1_t

                q_val_float = float(q_val.squeeze().cpu().item())


            # æŸäº›éæ³•èµ·ç‚¹å¯èƒ½è¢« reset æ—¶é‡é‡‡æ ·ï¼ˆæ¯”å¦‚è½åœ¨éšœç¢é‡Œä¸”å¼ºçº¦æŸï¼‰ï¼Œè¿™ä¼šå¯¼è‡´
            # obs å¯¹åº”çš„å®é™…èµ·ç‚¹ä¸è®¾å®šä¸åŒï¼Œä½†å¯¹â€œå…¨å±€å¯è¾¾æ€§è¿‘ä¼¼åœºâ€ä»æœ‰å‚è€ƒæ„ä¹‰ã€‚
            # å¦‚éœ€å¼ºåˆ¶ä¸¥æ ¼ä½¿ç”¨è¯¥ç‚¹ï¼Œå¯åœ¨ env.reset é‡Œæ”¾å®½/å…³é—­éšœç¢ç‚¹è¿‡æ»¤ï¼Œæˆ–è·³è¿‡è¿™äº›ç‚¹ã€‚
            Qmap[iy, ix] = float(q_val)  # æ³¨æ„è¡Œåˆ—ï¼šy ä¸ºè¡Œï¼Œx ä¸ºåˆ—

    # ---- ç»˜åˆ¶ç­‰å€¼é¢ ----
    plt.figure(figsize=(7, 7))
    ax = plt.gca()
    X, Y = np.meshgrid(xs, ys)

    # ä¸ºäº†å¯è¯»æ€§ï¼Œå¯åšç®€å•å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
    finite_vals = Qmap[np.isfinite(Qmap)]
    if finite_vals.size > 0:
        vmin, vmax = np.percentile(finite_vals, [5, 95])  # ç¨å¾®æŠ‘åˆ¶æç«¯å€¼
    else:
        vmin, vmax = None, None

    cs = ax.contourf(X, Y, Qmap, levels=30, cmap=cmap, vmin=vmin, vmax=vmax)
    cb = plt.colorbar(cs, ax=ax, shrink=0.8)
    cb.set_label("Q(s, Ï€(s)) (higher = better)", rotation=90)

    # ç”»éšœç¢ & å®‰å…¨åœˆ
    oc = getattr(base_env, "obstacle_center", (0.0, 0.0))
    orad = getattr(base_env, "obstacle_radius", 1.0)
    ax.add_patch(Circle(oc, radius=orad, facecolor="k", edgecolor="k", alpha=0.18, lw=1.0))
    safe_margin = getattr(base_env, "safe_margin", 0.0)
    if safe_margin > 0:
        ax.add_patch(Circle(oc, radius=orad + safe_margin, fill=False, ls="--", lw=1.0, ec="k", alpha=0.25))

    # æ ‡å‡ºç›®æ ‡ç‚¹ä¸æœå‘
    xg, yg, thg = goal_pose
    ax.plot([xg], [yg], marker="*", markersize=12, color="black", alpha=0.9)
    Lvis = 0.6
    ax.arrow(xg, yg, Lvis*np.cos(thg), Lvis*np.sin(thg),
             head_width=0.25, length_includes_head=True, color="black", alpha=0.7)

    ax.set_aspect("equal")
    ax.grid(True, ls="--", alpha=0.4)
    ax.set_xlabel("x (start)")
    ax.set_ylabel("y (start)")
    ax.set_title(f"{title}\nGoal=({xg:.2f},{yg:.2f},{thg:.2f}), start_theta={start_theta:.2f}")

    ts = time.strftime("%Y%m%d-%H%M%S")
    out_path = os.path.join(plot_dir, f"{ALG}_{ENV_ID}_critic_contour_{ts}.png")
    plt.savefig(out_path, dpi=180, bbox_inches="tight")
    print(f"[contour] Saved Q-field to: {out_path}")

    traj_venv.close()

# ============ è½¨è¿¹å¯è§†åŒ–ï¼ˆå¯å•ç‹¬è°ƒç”¨ï¼‰ ============

def visualize_trajectories(
    log_dir: str,
    alg_name: str = ALG,
    episodes: int = 16,
    max_steps: int = 400,
    deterministic: bool = True,
    title_suffix: str = "",
    # ğŸ‘‡ æ–°å¢ï¼šå¯é€‰å›ºå®šèµ·ç‚¹/ç»ˆç‚¹ï¼ˆå¯¹å…¨éƒ¨ episodes ç”Ÿæ•ˆï¼‰
    start_pose: tuple | None = None,  # (x, y, theta)
    goal_pose: tuple | None = None,   # (xg, yg, thetag)
    # ğŸ‘‡ å¯é€‰ï¼šä¹Ÿæ”¯æŒä¸ºæ¯æ¡è½¨è¿¹å‡†å¤‡ä¸€å¯¹èµ·ç»ˆç‚¹ï¼ˆé•¿åº¦éœ€ä¸ episodes ä¸€è‡´ï¼‰
    start_goals: list[tuple[tuple, tuple]] | None = None
):
    """
    ä» log_dir è¯»å– <ALG>_kinematic_model ä¸ <ALG>_vecnorm.pklï¼Œç”»å¤šæ¡è½¨è¿¹å¹¶ä¿å­˜åˆ° plots/ ä¸‹ã€‚
    """
    model_path = os.path.join(log_dir, f"{alg_name}_kinematic_model")
    vecnorm_path = os.path.join(log_dir, f"{alg_name}_vecnorm.pkl")
    plot_dir = os.path.join(log_dir, "plots")
    os.makedirs(plot_dir, exist_ok=True)

    # å•ç¯å¢ƒè¯„ä¼°/é‡‡æ ·
    traj_venv = DummyVecEnv([make_env_for_traj()])
    traj_venv = VecNormalize.load(vecnorm_path, traj_venv)
    traj_venv.training = False
    traj_venv.norm_reward = False

    device = "cuda" if torch.cuda.is_available() else "cpu"
    loaded = Algo.load(model_path, device=device)
    base_env = traj_venv.venv.envs[0].unwrapped

    def rollout_one_episode(max_steps_=max_steps, ep_idx=0):
        # å¦‚æœç”¨æˆ·æä¾›äº†å›ºå®šèµ·ç»ˆç‚¹/æˆ–æ¯é›†ä¸€å¯¹çš„åˆ—è¡¨ï¼Œåˆ™é€šè¿‡ env_method æ³¨å…¥
        if start_goals is not None:
            assert ep_idx < len(start_goals), "start_goals é•¿åº¦å¿…é¡» >= episodes"
            s, g = start_goals[ep_idx]
        else:
            s, g = start_pose, goal_pose

        if (s is not None) or (g is not None):
            # é€šè¿‡ VecNormalize/DummyVecEnv çš„ env_method ä¼ åˆ°åº•å±‚ç¯å¢ƒ
            traj_venv.env_method("set_start_goal_for_next_reset", start=s, goal=g)

        obs = traj_venv.reset()
        start = base_env.state
        goal = base_env.goal

        xs, ys, ths = [start[0]], [start[1]], [start[2]]
        steps = 0
        is_success = False

        for _ in range(max_steps_):
            action, _ = loaded.predict(obs, deterministic=deterministic)
            obs, rewards, dones, infos = traj_venv.step(action)
            info0 = infos[0]
            if dones[0]:
                term = info0.get("terminal_state", None)
                if term is not None:
                    xs.append(term[0]); ys.append(term[1]); ths.append(term[2])
                else:
                    x, y, th = base_env.state
                    xs.append(x); ys.append(y); ths.append(th)
                is_success = bool(info0.get("is_success", False))
                break
            else:
                st = info0.get("state", None)
                if st is not None:
                    xs.append(st[0]); ys.append(st[1]); ths.append(st[2])
                else:
                    x, y, th = base_env.state
                    xs.append(x); ys.append(y); ths.append(th)
            steps += 1
        return xs, ys, ths, start, goal, steps, is_success

    # æ”¶é›†å¤šæ¡è½¨è¿¹
    trajectories = []
    succ_cnt = 0
    for ep in range(episodes):
        xs, ys, ths, st, gl, steps, ok = rollout_one_episode(ep_idx=ep)
        trajectories.append((xs, ys, ths, st, gl, steps, ok))
        succ_cnt += int(ok)


    # ç»˜å›¾
    plt.figure(figsize=(7, 7))
    ax = plt.gca()
    # -------------------- ğŸ”¸é¢œè‰²ç”Ÿæˆç­–ç•¥--------------------
    # 1. æˆåŠŸè½¨è¿¹ç”¨ç»¿è‰²æ¸å˜ ('Greens')
    # 2. å¤±è´¥è½¨è¿¹ç”¨çº¢è‰²æ¸å˜ ('Reds')
    succ_trajs = [t for t in trajectories if t[-1] is True]
    fail_trajs = [t for t in trajectories if t[-1] is False]

    succ_cmap = cm.get_cmap('Greens', len(succ_trajs) + 2)
    fail_cmap = cm.get_cmap('Reds', len(fail_trajs) + 2)

    succ_colors = [succ_cmap(i + 1) for i in range(len(succ_trajs))]
    fail_colors = [fail_cmap(i + 1) for i in range(len(fail_trajs))]

    # ä¸ºç»Ÿä¸€é¡ºåºé‡ç»„è½¨è¿¹å’Œé¢œè‰²åˆ—è¡¨
    colors = []
    for t in trajectories:
        if t[-1]:
            colors.append(succ_colors.pop(0))
        else:
            colors.append(fail_colors.pop(0))
    # -----------------------------------------------------

    # ç”»éšœç¢ç‰© + å®‰å…¨åœˆ
    oc = getattr(base_env, "obstacle_center", (0.0, 0.0))
    orad = getattr(base_env, "obstacle_radius", 1.0)
    ax.add_patch(Circle(oc, radius=orad, facecolor="k", edgecolor="k", alpha=0.15, lw=1.0))
    safe_margin = getattr(base_env, "safe_margin", 0.0)
    if safe_margin > 0:
        ax.add_patch(Circle(oc, radius=orad + safe_margin, fill=False, ls="--", lw=1.0, ec="k", alpha=0.25))

    arrow_stride = 10
    arrow_len = 0.3

    for i, (xs, ys, ths, st, gl, steps, ok) in enumerate(trajectories):
        c = colors[i % len(colors)]
        lbl = f"ep{i+1} ({'âœ“' if ok else 'Ã—'}, {steps} steps)"
        plt.plot(xs, ys, '-', lw=2, color=c, alpha=0.9, label=lbl)
        plt.plot(xs[0], ys[0], 'o', color=c, ms=6, alpha=0.9)
        plt.plot(gl[0], gl[1], '*', color=c, ms=12, alpha=0.9)

        st_th = st[2]
        plt.arrow(st[0], st[1],
                  2*arrow_len*np.cos(st_th), 2*arrow_len*np.sin(st_th),
                  head_width=0.2, length_includes_head=True, color=c, alpha=0.9)
        gl_th = gl[2]
        plt.arrow(gl[0], gl[1],
                  2*arrow_len*np.cos(gl_th), 2*arrow_len*np.sin(gl_th),
                  head_width=0.2, length_includes_head=True, color=c, alpha=0.5)

        idx = np.arange(0, len(xs), arrow_stride)
        ux = arrow_len * np.cos(np.array(ths)[idx])
        uy = arrow_len * np.sin(np.array(ths)[idx])
        plt.quiver(np.array(xs)[idx], np.array(ys)[idx], ux, uy,
                   angles='xy', scale_units='xy', scale=1.0,
                   width=0.004, color=c, alpha=0.7)

    # æ”¾åœ¨ä¸‹æ–¹
    plt.legend(
        loc='upper center',
        bbox_to_anchor=(0.5, -0.08),
        ncol=4,            # åˆ† 4 åˆ—æ¨ªæ’
        frameon=False,
        fontsize=8
    )
    plt.tight_layout()

    plt.axis('equal'); plt.grid(True, ls='--', alpha=0.5)
    plt.xlabel('x'); plt.ylabel('y')
    ttl = f"{ENV_ID} trajectories ({ALG}) | {succ_cnt}/{episodes} reached"
    if title_suffix:
        ttl += f" | {title_suffix}"
    plt.title(ttl)

    ts = time.strftime("%Y%m%d-%H%M%S")
    out_path = os.path.join(plot_dir, f"{ALG}_{ENV_ID}_multi_traj_{episodes}_{ts}.png")
    plt.savefig(out_path, dpi=180, bbox_inches="tight")
    print(f"[plot] Saved trajectory figure to: {out_path}")

    traj_venv.close()

# ============ è®­ç»ƒä¸»æµç¨‹ ============

def train_and_optionally_eval(args):
    # çº¿ç¨‹é™åˆ¶
    torch.set_num_threads(1)
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS", "1")

    LOG_DIR = args.log_dir
    PLOT_DIR = os.path.join(LOG_DIR, "plots")
    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(PLOT_DIR, exist_ok=True)

    backup_sources(LOG_DIR)

    tmp = gym.make(ENV_ID)
    n_actions = tmp.action_space.shape[-1]
    tmp.close()

    venv = SubprocVecEnv([make_env_train(i, LOG_DIR) for i in range(args.n_envs)])
    venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0, gamma=0.99)

    sigma = np.array([0.5, 0.4], dtype=np.float32) if n_actions == 2 else 0.3 * np.ones(n_actions, dtype=np.float32)
    action_noise = NormalActionNoise(mean=np.zeros(n_actions, dtype=np.float32), sigma=sigma)

    model = Algo(
        "MlpPolicy",
        venv,
        device="cuda" if torch.cuda.is_available() else "cpu",
        learning_rate=3e-4,
        buffer_size=400_000,
        learning_starts=10_000,
        batch_size=512,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        action_noise=action_noise,
        tensorboard_log=LOG_DIR,
        seed=SEED,
        verbose=1,
        policy_kwargs=dict(net_arch=dict(pi=[256, 256, 256, 256, 256, 256],
                                         qf=[256, 256, 256, 256, 256, 256])),
    )

    eval_env = SubprocVecEnv([make_env_eval_for_callback(i, LOG_DIR) for i in range(args.eval_envs)])
    eval_env = VecNormalize(eval_env, training=False, norm_obs=True, norm_reward=False)
    sync_envs_normalization(venv, eval_env)

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(LOG_DIR, "best"),
        log_path=LOG_DIR,
        eval_freq=50_000,
        n_eval_episodes=100,
        deterministic=True,
    )

    print("[train] start")
    t0 = time.time()
    model.learn(total_timesteps=args.total_steps, log_interval=10, callback=eval_callback)
    t1 = time.time()
    mins, secs = divmod(t1 - t0, 60)
    print(f"[train] done in {int(mins)}m{secs:.1f}s")

    model_path = os.path.join(LOG_DIR, f"{ALG}_kinematic_model")
    vecnorm_path = os.path.join(LOG_DIR, f"{ALG}_vecnorm.pkl")
    model.save(model_path)
    venv.save(vecnorm_path)
    print(f"Saved model to: {model_path}")
    print(f"Saved VecNormalize stats to: {vecnorm_path}")

    # ç®€å• evalï¼ˆå¯é€‰ï¼‰
    eval_venv = DummyVecEnv([make_env_eval()])
    eval_venv = VecNormalize.load(vecnorm_path, eval_venv)
    eval_venv.training = False
    eval_venv.norm_reward = False
    mean_r, std_r = evaluate_policy(model, eval_venv, n_eval_episodes=10, deterministic=True)
    print(f"[{ALG}] Eval return: {mean_r:.2f} Â± {std_r:.2f}")

# ============ CLI å…¥å£ ============

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--mode", choices=["train", "plot", "train_and_plot", "contour"], default="train",
                   help="train: åªè®­ç»ƒ; plot: åªç”»å›¾; train_and_plot: è®­ç»ƒå®Œç«‹åˆ»ç”»ä¸€æ¬¡; contour: ç”»å‡ºç­‰åŠ¿çº¿")
    # contour ä¸“ç”¨
    p.add_argument("--goal", type=str, default=None, help='ç›®æ ‡ "xg,yg,thg"ï¼ˆå¿…å¡«ï¼‰')
    p.add_argument("--x-range", type=str, default="-6,6", help='èµ·ç‚¹ x é‡‡æ ·èŒƒå›´ "xmin,xmax"')
    p.add_argument("--y-range", type=str, default="-6,6", help='èµ·ç‚¹ y é‡‡æ ·èŒƒå›´ "ymin,ymax"')
    p.add_argument("--grid-N", type=int, default=81, help="ç½‘æ ¼å¯†åº¦ï¼ˆå»ºè®®å¥‡æ•°ï¼‰")
    p.add_argument("--start-theta", type=float, default=0.0, help="èµ·ç‚¹æœå‘")
    
    # åŸºæœ¬å‚æ•°
    p.add_argument("--log-dir", type=str,
                   default="/workspace/kinodynamic_car_SB3/backup/with_obstacle_avoidance_v7/0",
                   help="ä¿å­˜/è¯»å–æ¨¡å‹çš„ç›®å½•")
    p.add_argument("--n-envs", type=int, default=N_ENVS, help="å¹¶è¡Œç¯å¢ƒä¸ªæ•°")
    p.add_argument("--eval-envs", type=int, default=EVAL_ENVS, help="è¯„ä¼°å¹¶è¡Œç¯å¢ƒä¸ªæ•°")
    p.add_argument("--total-steps", type=int, default=TOTAL_STEPS, help="è®­ç»ƒæ€»æ­¥æ•°")

    # å¯è§†åŒ–ä¸“ç”¨å‚æ•°
    p.add_argument("--episodes", type=int, default=16, help="å¯è§†åŒ–çš„è½¨è¿¹æ¡æ•°")
    p.add_argument("--max-steps", type=int, default=400, help="æ¯æ¡è½¨è¿¹çš„æœ€å¤§æ­¥æ•°")
    p.add_argument("--deterministic", action="store_true", help="å¯è§†åŒ–æ—¶ç”¨ç¡®å®šæ€§ç­–ç•¥")
    return p.parse_args()

if __name__ == "__main__":
    import multiprocessing as mp
    try:
        mp.set_start_method("spawn", force=True)
    except RuntimeError:
        pass

    def _parse_triplet(s: str):
        # "a,b,c" -> (a, b, c)
        parts = [float(x) for x in s.split(",")]
        if len(parts) != 3:
            raise ValueError("éœ€è¦ 3 ä¸ªæ•°ï¼Œä¾‹å¦‚: --goal \"5.0,4.0,1.57\"")
        return tuple(parts)

    def _parse_pair(s: str):
        # "a,b" -> (a, b)
        parts = [float(x) for x in s.split(",")]
        if len(parts) != 2:
            raise ValueError("éœ€è¦ 2 ä¸ªæ•°ï¼Œä¾‹å¦‚: --x-range \"-6,6\"")
        return tuple(parts)

    args = parse_args()

    if args.mode in ["train", "train_and_plot"]:
        train_and_optionally_eval(args)

    if args.mode in ["plot", "train_and_plot"]:
        starts_goals = [
            ((-4.0, -4.0, 0.0), (4.0, 1, 0.0)),
            ((-4.0, -4.0, 0.0), (4.0, 1.5, 0.0)),
            ((-4.0, -4.0, 0.0), (4.0, 2, 0.0)),
            ((-4.0, -4.0, 0.0), (4.0, 2.5, 0.0)),
            ((-4.0, -4.0, 0.0), (4.0, 3, 0.0)),
            ((-4.0, -4.0, 0.0), (4.0, 3.5, 0.0)),
        ]
        visualize_trajectories(
            log_dir=args.log_dir,
            alg_name=ALG,
            # episodes=args.episodes,
            max_steps=args.max_steps,
            deterministic=args.deterministic,
            title_suffix=args.mode,

            start_goals=starts_goals,
            episodes=len(starts_goals),

            # start_pose=(-4.0, -4.0, 0.0),    # èµ·ç‚¹ (x, y, Î¸)
            # goal_pose=(2.0, 1.5, 0.0),      # ç»ˆç‚¹ (x, y, Î¸)
        )
        
    if args.mode == "contour":
        if args.goal is None:
            raise SystemExit("å¿…é¡»æä¾› --goal \"xg,yg,thg\"")
        goal_pose = _parse_triplet(args.goal)
        x_rng = _parse_pair(args["x_range"] if isinstance(args, dict) else args.x_range)
        y_rng = _parse_pair(args["y_range"] if isinstance(args, dict) else args.y_range)

        visualize_critic_contour(
            log_dir=args.log_dir,
            goal_pose=goal_pose,
            x_range=x_rng,
            y_range=y_rng,
            grid_N=args.grid_N,
            start_theta=args.start_theta,
            deterministic=True,
        )

